#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ åŸºäºæ—¶é—´çª—å£+è¡°å‡æƒé‡çš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½:
1. æ—¶é—´çª—å£å†…è®¡ç®—å…¨å±€TF-IDF
2. æ„å»ºå¸¦æ—¶é—´è¡°å‡çš„å‘é‡
3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€â‰¥0.6ï¼‰

Version: 1.0
Date: 2026-02-11
"""

import math
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import numpy as np


class TimeWeightedRetriever:
    """
    åŸºäºæ—¶é—´çª—å£+è¡°å‡æƒé‡çš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ
    """
    
    def __init__(self, window_hours: int = 24, decay_rate: float = 0.1):
        """
        åˆå§‹åŒ–
        
        Args:
            window_hours: æ—¶é—´çª—å£å¤§å°ï¼ˆå°æ—¶ï¼‰
            decay_rate: æ—¶é—´è¡°å‡ç‡
        """
        self.window_hours = window_hours
        self.decay_rate = decay_rate
        self.documents = []  # æ–‡æ¡£åˆ—è¡¨
        self.vocabulary = set()  # è¯æ±‡è¡¨
        self.idf = {}  # IDFå€¼
        self.index = defaultdict(list)  # å€’æ’ç´¢å¼•
        
    def add_document(self, doc_id: str, content: str, timestamp: datetime):
        """
        æ·»åŠ æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
            content: æ–‡æ¡£å†…å®¹
            timestamp: æ—¶é—´æˆ³
        """
        # é¢„å¤„ç†
        tokens = self._preprocess(content)
        
        # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
        self.documents.append({
            'id': doc_id,
            'tokens': tokens,
            'timestamp': timestamp,
            'tf': Counter(tokens)
        })
        
        # æ›´æ–°è¯æ±‡è¡¨
        self.vocabulary.update(tokens)
        
        # æ›´æ–°å€’æ’ç´¢å¼•
        for token in set(tokens):
            self.index[token].append(doc_id)
        
        # æ ‡è®°éœ€è¦é‡æ–°è®¡ç®—IDF
        self._idf_stale = True
    
    def _preprocess(self, text: str) -> List[str]:
        """
        æ–‡æ¡£é¢„å¤„ç†
        
        Steps:
        1. è½¬å°å†™
        2. åˆ†è¯
        3. å»åœç”¨è¯
        4. è¯å¹²æå–
        """
        # ç®€åŒ–ï¼šåªåšåŸºæœ¬å¤„ç†
        tokens = text.lower().split()
        return [token.strip('.,!?()[]') for token in tokens]
    
    def compute_global_tf_idf(self):
        """
        Step 1: åœ¨æ—¶é—´çª—å£å†…è®¡ç®—å…¨å±€TF-IDF
        
        æ ¸å¿ƒæ­¥éª¤:
        1. ç­›é€‰çª—å£å†…æ–‡æ¡£
        2. è®¡ç®—TFï¼ˆè¯é¢‘ï¼‰
        3. è®¡ç®—IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
        4. è®¡ç®—TF-IDFæƒé‡
        """
        # Step 1: ç­›é€‰æ—¶é—´çª—å£å†…çš„æ–‡æ¡£
        now = datetime.now()
        window_start = now - timedelta(hours=self.window_hours)
        window_docs = [
            doc for doc in self.documents 
            if doc['timestamp'] >= window_start
        ]
        
        if not window_docs:
            print("âš ï¸ çª—å£å†…æ— æ–‡æ¡£")
            return
        
        print(f"ğŸ“Š çª—å£å†…æ–‡æ¡£æ•°: {len(window_docs)}")
        
        # Step 2: è®¡ç®—IDF
        # IDF = log(N / df)
        N = len(window_docs)
        self.idf = {}
        
        for token in self.vocabulary:
            df = len(set(doc['id'] for doc in window_docs if token in doc['tokens']))
            if df > 0:
                self.idf[token] = math.log(N / df)
            else:
                self.idf[token] = 0
        
        self._idf_stale = False
        
        # Step 3: è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„TF-IDFå‘é‡
        for doc in window_docs:
            doc['tfidf'] = {}
            max_tf = max(doc['tf'].values()) if doc['tf'] else 1
            
            for token in self.vocabulary:
                # TF-IDF = TF * IDF
                tf = doc['tf'].get(token, 0) / max_tf
                tfidf = tf * self.idf.get(token, 0)
                doc['tfidf'][token] = tfidf
        
        print(f"âœ… TF-IDFè®¡ç®—å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
    
    def _compute_time_decay(self, doc_timestamp: datetime) -> float:
        """
        Step 2: è®¡ç®—æ—¶é—´è¡°å‡æƒé‡
        
        è¡°å‡å…¬å¼:
        weight = e^(-decay_rate * hours_ago)
        
        æœ€è¿‘æ–‡æ¡£æƒé‡ â†’ 1
        è¶Šä¹…è¿œæƒé‡ â†’ 0
        """
        now = datetime.now()
        hours_ago = (now - doc_timestamp).total_seconds() / 3600
        
        # æŒ‡æ•°è¡°å‡
        weight = math.exp(-self.decay_rate * hours_ago)
        
        return weight
    
    def build_weighted_vector(self, doc_id: str) -> Dict[str, float]:
        """
        Step 3: æ„å»ºå¸¦æ—¶é—´è¡°å‡çš„å‘é‡
        
        å…¬å¼:
        weighted_tfidf = TF-IDF * time_decay
        
        æ ¸å¿ƒæ€æƒ³:
        - æ–°æ–‡æ¡£æƒé‡é«˜
        - æ—§æ–‡æ¡£æƒé‡ä½
        """
        # è·å–æ–‡æ¡£
        doc = next((d for d in self.documents if d['id'] == doc_id), None)
        if not doc:
            return {}
        
        # è®¡ç®—æ—¶é—´è¡°å‡
        decay = self._compute_time_decay(doc['timestamp'])
        
        # åº”ç”¨è¡°å‡æƒé‡
        weighted_vector = {}
        for token in self.vocabulary:
            tfidf = doc['tfidf'].get(token, 0)
            weighted_vector[token] = tfidf * decay
        
        return weighted_vector
    
    def cosine_similarity(self, vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """
        Step 4: è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        
        å…¬å¼:
        cos(Î¸) = (AÂ·B) / (||A|| Ã— ||B||)
        
        è¦æ±‚: â‰¥ 0.6
        """
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(vec1.get(token, 0) * vec2.get(token, 0) 
                        for token in set(vec1.keys()) | set(vec2.keys()))
        
        # è®¡ç®—èŒƒæ•°
        norm1 = math.sqrt(sum(v**2 for v in vec1.values()))
        norm2 = math.sqrt(sum(v**2 for v in vec2.values()))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = dot_product / (norm1 * norm2)
        
        return similarity
    
    def search(self, query: str, threshold: float = 0.6) -> List[Tuple[str, float]]:
        """
        Step 5: æ£€ç´¢
        
        æ ¸å¿ƒæ­¥éª¤:
        1. è®¡ç®—queryçš„TF-IDFå‘é‡
        2. æ„å»ºå¸¦æ—¶é—´è¡°å‡çš„æ–‡æ¡£å‘é‡
        3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        4. è¿‡æ»¤å¹¶æ’åºç»“æœ
        """
        # ç¡®ä¿TF-IDFå·²è®¡ç®—
        if self._idf_stale or not self.idf:
            self.compute_global_tf_idf()
        
        # Step 1: å¤„ç†query
        query_tokens = self._preprocess(query)
        query_counter = Counter(query_tokens)
        max_tf = max(query_counter.values()) if query_counter else 1
        
        # æ„å»ºqueryå‘é‡ï¼ˆå¸¦IDFæƒé‡ï¼‰
        query_vector = {}
        for token in self.vocabulary:
            tf = query_counter.get(token, 0) / max_tf
            query_vector[token] = tf * self.idf.get(token, 0)
        
        # Step 2: è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„åŠ æƒå‘é‡
        results = []
        
        for doc in self.documents:
            # æ„å»ºå¸¦æ—¶é—´è¡°å‡çš„å‘é‡
            doc_vector = self.build_weighted_vector(doc['id'])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = self.cosine_similarity(query_vector, doc_vector)
            
            # è¿‡æ»¤é˜ˆå€¼
            if similarity >= threshold:
                results.append((doc['id'], similarity))
        
        # æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results


def demo():
    """æ¼”ç¤º"""
    print("="*70)
    print("ğŸ¦ åŸºäºæ—¶é—´çª—å£+è¡°å‡æƒé‡çš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ")
    print("="*70)
    
    # åˆå§‹åŒ–ï¼ˆçª—å£24å°æ—¶ï¼Œè¡°å‡ç‡0.1ï¼‰
    retriever = TimeWeightedRetriever(window_hours=24, decay_rate=0.1)
    
    # æ·»åŠ æµ‹è¯•æ–‡æ¡£
    now = datetime.now()
    
    documents = [
        ("doc1", "Python programming language", now - timedelta(hours=2)),
        ("doc2", "Machine learning algorithms", now - timedelta(hours=5)),
        ("doc3", "Deep neural networks", now - timedelta(hours=12)),
        ("doc4", "Natural language processing", now - timedelta(hours=23)),
        ("doc5", "Computer vision techniques", now - timedelta(hours=48)),  # çª—å£å¤–
    ]
    
    print("\nğŸ“„ æ·»åŠ æ–‡æ¡£:")
    for doc_id, content, timestamp in documents:
        hours_ago = (now - timestamp).total_seconds() / 3600
        retriever.add_document(doc_id, content, timestamp)
        print(f"  {doc_id}: {content} ({hours_ago:.1f}å°æ—¶å‰)")
    
    # è®¡ç®—TF-IDF
    print("\nğŸ“Š Step 1: è®¡ç®—å…¨å±€TF-IDF")
    retriever.compute_global_tf_idf()
    
    # æ£€ç´¢
    print("\nğŸ” Step 2: æ£€ç´¢ 'neural networks'")
    results = retriever.search("neural networks", threshold=0.6)
    
    print(f"\nç»“æœ (ç›¸ä¼¼åº¦ â‰¥ 0.6):")
    for doc_id, similarity in results:
        print(f"  {doc_id}: {similarity:.4f}")
    
    # å±•ç¤ºæ—¶é—´è¡°å‡
    print("\nâ° æ—¶é—´è¡°å‡ç¤ºä¾‹:")
    for hours in [0, 1, 5, 10, 24, 48]:
        weight = math.exp(-0.1 * hours)
        print(f"  {hours:2d}å°æ—¶å‰: {weight:.4f}")
    
    print("\n" + "="*70)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("="*70)


if __name__ == "__main__":
    demo()
