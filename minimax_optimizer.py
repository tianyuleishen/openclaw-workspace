#!/usr/bin/env python3
"""
MiniMax æ¨¡å‹è°ƒç”¨ä¼˜åŒ–å™¨
æ ¹æ® Coding Plan é™åˆ¶ï¼Œæœ€å¤§åŒ–æ¨¡å‹èµ„æºåˆ©ç”¨

é™åˆ¶è¯´æ˜ï¼š
- 1 prompt â‰ˆ 15æ¬¡æ¨¡å‹è°ƒç”¨ï¼ˆæ‰“åŒ…è®¡è´¹ï¼‰
- æ¯5å°æ—¶é‡ç½®é€Ÿç‡é™åˆ¶
- API Key åŒºåˆ†ï¼šCoding Plan vs æ™®é€š
- é¢åº¦å¤šå·¥å…·å…±äº«
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from collections import deque


@dataclass
class UsageRecord:
    """ä½¿ç”¨è®°å½•"""
    timestamp: datetime
    prompt_count: int
    task_type: str
    tokens_used: int


class MiniMaxOptimizer:
    """
    MiniMax æ¨¡å‹è°ƒç”¨ä¼˜åŒ–å™¨
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. æ‰¹é‡å¤„ç† - åˆå¹¶å¤šä¸ªå°ä»»åŠ¡ä¸ºä¸€æ¬¡ prompt
    2. æ™ºèƒ½ç¼“å­˜ - é¿å…é‡å¤è°ƒç”¨ç›¸åŒå†…å®¹
    3. ä¼˜å…ˆçº§é˜Ÿåˆ— - é‡è¦ä»»åŠ¡ä¼˜å…ˆ
    4. æ··åˆä½¿ç”¨ - Coding Plan vs æ™®é€š Key
    5. é€Ÿç‡æ§åˆ¶ - é¿å…è§¦å‘5å°æ—¶é™é¢
    """

    def __init__(self, coding_plan_key: str = None, normal_key: str = None):
        # API Keys
        self.coding_key = coding_plan_key
        self.normal_key = normal_key
        
        # ä½¿ç”¨è®°å½•ï¼ˆæœ€è¿‘5å°æ—¶ï¼‰
        self.usage_history: deque = deque(maxlen=1000)
        
        # ç¼“å­˜
        self.response_cache: Dict[str, Dict] = {}
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜
        
        # é™åˆ¶é…ç½®
        self.limits = {
            'prompt_per_5h': 1000,  # å‡è®¾çš„ Coding Plan é™é¢
            'cache_size': 1000,
            'batch_size': 5  # æ¯æ¬¡æ‰¹é‡å¤„ç†çš„ä»»åŠ¡æ•°
        }
        
        # ç»Ÿè®¡
        self.stats = {
            'total_prompts': 0,
            'total_tokens': 0,
            'cache_hits': 0,
            'batches_processed': 0,
            'saved_prompts': 0
        }

    # ==================== æ ¸å¿ƒä¼˜åŒ–æ–¹æ³• ====================

    def should_use_coding_plan(self, task_type: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Coding Plan
        
        ç­–ç•¥ï¼š
        - ç¼–ç¨‹ç›¸å…³ä»»åŠ¡ â†’ Coding Plan
        - ç®€å•å¯¹è¯ â†’ æ™®é€š Key
        - å¤§æ‰¹é‡å¤„ç† â†’ ç¼“å­˜/æ‰¹é‡
        """
        coding_tasks = ['code', 'debug', 'refactor', 'review', 'explain_code']
        simple_tasks = ['chat', 'greeting', 'simple_qa']
        
        if any(t in task_type.lower() for t in coding_tasks):
            return True
        elif any(t in task_type.lower() for t in simple_tasks):
            return False
        else:
            # é»˜è®¤ä½¿ç”¨ Coding Planï¼ˆå¦‚æœæ˜¯ç¼–ç¨‹å¥—é¤ï¼‰
            return self.coding_key is not None

    def batch_requests(self, requests: List[Dict]) -> List[List[Dict]]:
        """
        æ‰¹é‡å¤„ç†è¯·æ±‚
        
        å°†å¤šä¸ªå°è¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ª batchï¼Œå‡å°‘ prompt æ¬¡æ•°
        """
        batches = []
        for i in range(0, len(requests), self.limits['batch_size']):
            batch = requests[i:i + self.limits['batch_size']]
            batches.append(batch)
        
        self.stats['batches_processed'] += len(batches)
        return batches

    def get_cached_response(self, content_hash: str) -> Optional[str]:
        """è·å–ç¼“å­˜å“åº”"""
        if content_hash in self.response_cache:
            record = self.response_cache[content_hash]
            if (datetime.now() - record['timestamp']).seconds < self.cache_ttl:
                self.stats['cache_hits'] += 1
                return record['response']
        return None

    def cache_response(self, content_hash: str, response: str):
        """ç¼“å­˜å“åº”"""
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        if len(self.response_cache) >= self.limits['cache_size']:
            # ç§»é™¤æœ€æ—§çš„
            oldest = min(self.response_cache.keys(), 
                        key=lambda k: self.response_cache[k]['timestamp'])
            del self.response_cache[oldest]
        
        self.response_cache[content_hash] = {
            'response': response,
            'timestamp': datetime.now()
        }

    def calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        import hashlib
        return hashlib.md5(content.encode()).hexdigest()[:16]

    # ==================== é€Ÿç‡æ§åˆ¶ ====================

    def get_5h_usage(self) -> int:
        """è·å–æœ€è¿‘5å°æ—¶çš„ä½¿ç”¨é‡"""
        cutoff = datetime.now() - timedelta(hours=5)
        total = sum(r.prompt_count for r in self.usage_history 
                   if r.timestamp > cutoff)
        return total

    def get_usage_percentage(self) -> float:
        """è·å–ä½¿ç”¨ç™¾åˆ†æ¯”"""
        used = self.get_5h_usage()
        return (used / self.limits['prompt_per_5h']) * 100

    def should_rate_limit(self) -> bool:
        """æ˜¯å¦åº”è¯¥é™æµ"""
        return self.get_5h_usage() >= self.limits['prompt_per_5h'] * 0.9

    def wait_if_needed(self):
        """å¦‚æœæ¥è¿‘é™é¢ï¼Œç­‰å¾…é‡ç½®"""
        if self.should_rate_limit():
            # è®¡ç®—ç­‰å¾…æ—¶é—´
            oldest = min(self.usage_history, 
                        key=lambda r: r.timestamp, default=None)
            if oldest:
                wait_seconds = (oldest.timestamp + timedelta(hours=5)) - datetime.now()
                if wait_seconds.total_seconds() > 0:
                    print(f"âš ï¸ æ¥è¿‘é™é¢ï¼Œç­‰å¾… {wait_seconds.total_seconds():.0f} ç§’...")
                    time.sleep(min(wait_seconds.total_seconds(), 300))  # æœ€å¤šç­‰5åˆ†é’Ÿ

    # ==================== ä¼˜åŒ–æ‰§è¡Œ ====================

    def optimize_request(self, task: Dict) -> Dict:
        """
        ä¼˜åŒ–å•ä¸ªè¯·æ±‚
        
        Returns:
            {
                'use_coding_plan': bool,
                'cached': bool,
                'batch_with': List[task_ids],
                'priority': int
            }
        """
        content = task.get('content', '')
        task_type = task.get('type', 'general')
        
        # 1. æ£€æŸ¥ç¼“å­˜
        content_hash = self.calculate_content_hash(content)
        cached = self.get_cached_response(content_hash)
        
        if cached:
            return {
                'use_coding_plan': False,
                'cached': True,
                'response': cached,
                'priority': 0
            }
        
        # 2. åˆ¤æ–­ä½¿ç”¨å“ªä¸ª Key
        use_coding = self.should_use_coding_plan(task_type)
        
        # 3. æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if use_coding and self.should_rate_limit():
            self.wait_if_needed()
        
        # 4. è¿”å›ä¼˜åŒ–å»ºè®®
        return {
            'use_coding_plan': use_coding,
            'cached': False,
            'content_hash': content_hash,
            'priority': 1 if 'urgent' in task else 2
        }

    def process_batch(self, tasks: List[Dict]) -> Dict:
        """
        å¤„ç†æ‰¹é‡ä»»åŠ¡
        
        ç­–ç•¥ï¼š
        1. å…ˆå¤„ç†ç¼“å­˜å‘½ä¸­çš„
        2. å‰©ä½™çš„åˆå¹¶ä¸º batch
        3. ä½¿ç”¨ Coding Plan æ‰¹é‡è°ƒç”¨
        """
        results = []
        to_process = []
        
        for i, task in enumerate(tasks):
            optimization = self.optimize_request(task)
            
            if optimization.get('cached'):
                results.append({
                    'task_id': i,
                    'response': optimization['response'],
                    'cached': True
                })
                self.stats['saved_prompts'] += 1
            else:
                to_process.append({
                    'task_id': i,
                    'optimization': optimization,
                    'original_task': task
                })
        
        # æ‰¹é‡å¤„ç†å‰©ä½™ä»»åŠ¡
        if to_process:
            batch_result = self._call_batch([
                t['original_task'] for t in to_process
            ])
            
            for i, result in enumerate(batch_result):
                task_info = to_process[i]
                results.append({
                    'task_id': task_info['task_id'],
                    'response': result,
                    'cached': False
                })
                
                # ç¼“å­˜ç»“æœ
                content_hash = task_info['optimization'].get('content_hash')
                if content_hash:
                    self.cache_response(content_hash, result)
        
        return results

    def _call_batch(self, tasks: List[Dict]) -> List[str]:
        """æ‰¹é‡è°ƒç”¨æ¨¡å‹"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„ MiniMax API
        # è¿”å›å“åº”åˆ—è¡¨
        pass

    # ==================== ç»Ÿè®¡ä¸æŠ¥å‘Š ====================

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'usage_5h': self.get_5h_usage(),
            'usage_percent': self.get_usage_percentage(),
            'total_prompts': self.stats['total_prompts'],
            'total_tokens': self.stats['total_tokens'],
            'cache_hits': self.stats['cache_hits'],
            'batches_processed': self.stats['batches_processed'],
            'saved_prompts': self.stats['saved_prompts'],
            'cache_size': len(self.response_cache),
            'efficiency': self._calculate_efficiency()
        }

    def _calculate_efficiency(self) -> float:
        """è®¡ç®—æ•ˆç‡"""
        total = self.stats['total_prompts'] + self.stats['saved_prompts']
        if total == 0:
            return 0
        return (self.stats['saved_prompts'] / total) * 100

    def generate_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        stats = self.get_stats()
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         MiniMax æ¨¡å‹è°ƒç”¨ä¼˜åŒ–æŠ¥å‘Š                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š 5å°æ—¶ä½¿ç”¨æƒ…å†µ
   ä½¿ç”¨é‡: {stats['usage_5h']} prompts
   ä½¿ç”¨ç‡: {stats['usage_percent']:.1f}%
   é™é¢: {self.limits['prompt_per_5h']}/5h

ğŸ“ˆ ä¼˜åŒ–æ•ˆæœ
   æ€»è°ƒç”¨: {stats['total_prompts']}
   ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}
   æ‰¹é‡å¤„ç†: {stats['batches_processed']}
   èŠ‚çœæ¬¡æ•°: {stats['saved_prompts']}
   æ•ˆç‡æå‡: {stats['efficiency']:.1f}%

ğŸ’¾ ç¼“å­˜çŠ¶æ€
   ç¼“å­˜å¤§å°: {stats['cache_size']}
   TTL: {self.cache_ttl}s

ğŸ¤– API Key ä½¿ç”¨
   Coding Plan: {'âœ… å·²é…ç½®' if self.coding_key else 'âŒ æœªé…ç½®'}
   æ™®é€š Key: {'âœ… å·²é…ç½®' if self.normal_key else 'âŒ æœªé…ç½®'}

ğŸ’¡ ä¼˜åŒ–å»ºè®®
"""
        
        if stats['usage_percent'] > 80:
            report += "   âš ï¸ ä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œå»ºè®®ï¼š\n"
            report += "   - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜\n"
            report += "   - é™ä½è°ƒç”¨é¢‘ç‡\n"
            report += "   - è€ƒè™‘åˆ‡æ¢åˆ°æŒ‰é‡ä»˜è´¹\n"
        else:
            report += "   âœ… ä½¿ç”¨ç‡æ­£å¸¸ï¼Œç»§ç»­ä¿æŒ\n"
        
        return report


# ==================== å¿«æ·å‡½æ•° ====================

_optimizer = None


def get_optimizer(coding_key: str = None, normal_key: str = None) -> MiniMaxOptimizer:
    global _optimizer
    if _optimizer is None:
        _optimizer = MiniMaxOptimizer(coding_key, normal_key)
    return _optimizer


def optimize_minimax_usage(tasks: List[Dict]) -> Dict:
    """
    ä¼˜åŒ– MiniMax æ¨¡å‹ä½¿ç”¨
    
    Usage:
        results = optimize_minimax_usage([
            {'type': 'code', 'content': 'Write a function...'},
            {'type': 'explain', 'content': 'Explain this code...'}
        ])
    """
    optimizer = get_optimizer()
    return optimizer.process_batch(tasks)


# æµ‹è¯•
if __name__ == "__main__":
    print("Testing MiniMax Optimizer...")
    
    optimizer = MiniMaxOptimizer()
    
    # æµ‹è¯•ä»»åŠ¡
    test_tasks = [
        {'type': 'code', 'content': 'def hello(): pass'},
        {'type': 'chat', 'content': 'Hello!'},
        {'type': 'debug', 'content': 'Fix this bug'},
        {'type': 'explain', 'content': 'Explain Python'},
        {'type': 'review', 'content': 'Review my code'},
    ]
    
    # æ‰¹é‡å¤„ç†
    results = optimizer.process_batch(test_tasks)
    
    print(f"\nå¤„ç†äº† {len(results)} ä¸ªä»»åŠ¡")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print(optimizer.generate_report())
    
    print("\nâœ… MiniMax Optimizer working!")
