#!/usr/bin/env python3
"""
ç»ˆææŒ‘æˆ˜ benchmark v12.0 (æœ€ç»ˆç‰ˆ)
"""

import sys
sys.path.insert(0, '/home/admin/.openclaw/workspace')

from reasoning_engine_v12_final import ReasoningEngineV12


def run_benchmark():
    print("="*70)
    print("ğŸ¦„ ç»ˆææŒ‘æˆ˜ benchmark v12.0 (æœ€ç»ˆç‰ˆ)")
    print("="*70)
    
    engine = ReasoningEngineV12()
    
    test_cases = [
        # v12.0æ–°å¢
        ("è§£é‡Šé»æ›¼Î¶å‡½æ•°çš„éå¹³å‡¡é›¶ç‚¹åˆ†å¸ƒçŒœæƒ³", "math_ultimate", ["é»æ›¼"]),
        ("è´¹é©¬å¤§å®šç† x^n + y^n = z^n è¯·è§£é‡Š", "math_ultimate", ["è´¹é©¬"]),
        ("P vs NPé—®é¢˜ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ", "math_ultimate", ["P", "NP"]),
        ("åº·æ‰˜å°”å¯¹è§’çº¿è®ºè¯å®æ•°ä¸å¯åˆ—", "math_ultimate", ["åº·æ‰˜å°”"]),
        ("ç´ æ•°æœ‰æ— ç©·å¤šä¸ªæ€ä¹ˆè¯æ˜ï¼Ÿ", "math_ultimate", ["ç´ æ•°"]),
        ("é‡å­çº ç¼ å’Œå åŠ æ€çš„åŒºåˆ«ï¼Ÿè´å°”ä¸ç­‰å¼ï¼Ÿ", "quantum", ["é‡å­"]),
        ("Shorç®—æ³•å¦‚ä½•åˆ†è§£å¤§æ•°ï¼Ÿå¯¹RSAå¨èƒï¼Ÿ", "quantum", ["Shor"]),
        ("Transformeræ³¨æ„åŠ›æœºåˆ¶è®¡ç®—è¿‡ç¨‹ï¼Ÿ", "ml_ultimate", ["Transformer"]),
        ("GPT-4å’ŒGPT-3.5åŒºåˆ«ï¼ŸScaling Lawï¼Ÿ", "ml_ultimate", ["GPT"]),
        ("ç¼¸ä¸­ä¹‹è„‘å¦‚ä½•è¯æ˜ä¸æ˜¯æ¨¡æ‹Ÿï¼Ÿ", "philosophy", ["ç¼¸ä¸­ä¹‹è„‘"]),
        ("ç”µè½¦éš¾é¢˜çš„åŠŸåˆ©ä¸»ä¹‰ vs ä¹‰åŠ¡è®º", "philosophy", ["ç”µè½¦"]),
        ("é«˜å¯ç”¨åˆ†å¸ƒå¼ç³»ç»Ÿå…³é”®ç»„ä»¶ï¼ŸCAPå®šç†ï¼Ÿ", "system_design", ["é«˜å¯ç”¨"]),
        ("äº‹ä»¶é©±åŠ¨å¾®æœåŠ¡æ¶æ„Pythonå®ç°", "system_design", ["äº‹ä»¶é©±åŠ¨"]),
        ("æœ‰æ•ˆå¸‚åœºå‡è¯´å’Œè¡Œä¸ºé‡‘èå­¦çš„å†²çª", "economics", ["æœ‰æ•ˆå¸‚åœº"]),
        ("IS-LMå’ŒAS-ADæ¨¡å‹åŒºåˆ«ï¼Ÿ", "economics", ["IS-LM"]),
        
        # v11.0åŸæœ‰
        ("è¯æ˜æ¬§æ‹‰å…¬å¼ e^(iÏ€) + 1 = 0", "math_advanced", ["æ¬§æ‹‰"]),
        ("ç”¨äºŒåˆ†æŸ¥æ‰¾åœ¨æœ‰åºæ•°ç»„ä¸­æ‰¾ç›®æ ‡å€¼", "coding_advanced", ["äºŒåˆ†æŸ¥æ‰¾"]),
        ("åŠå›æ›´å°½ä¸€æ¯é…’ï¼Œè¥¿å‡ºé˜³å…³æ— æ•…äºº", "poem_advanced", ["è¥¿å‡ºé˜³å…³"]),
    ]
    
    print(f"\nğŸ¦„ æµ‹è¯• {len(test_cases)} é“ç»ˆææŒ‘æˆ˜:")
    print("-"*70)
    
    results = {"passed": 0, "failed": 0, "by_type": {}}
    
    for i, (question, expected_type, keywords) in enumerate(test_cases, 1):
        result = engine.analyze(question)
        
        has_keyword = any(kw in result["answer"] for kw in keywords)
        matched = expected_type == result.get("type") or has_keyword
        
        status = "âœ…" if matched else "âŒ"
        
        if matched:
            results["passed"] += 1
        else:
            results["failed"] += 1
        
        print(f"{i:2d}. {result.get('type', 'general'):15s} {status} | {question[:35]:35s}")
        
        if expected_type not in results["by_type"]:
            results["by_type"][expected_type] = {"total": 0, "passed": 0}
        results["by_type"][expected_type]["total"] += 1
        if matched:
            results["by_type"][expected_type]["passed"] += 1
    
    total = len(test_cases)
    score = (results["passed"] / total) * 100
    
    print("\n" + "="*70)
    print("ğŸ“ˆ ç»ˆææŒ‘æˆ˜æ±‡æ€» v12.0 (æœ€ç»ˆç‰ˆ)")
    print("="*70)
    
    print(f"\næ€»é¢˜æ•°: {total}")
    print(f"é€šè¿‡: {results['passed']}")
    print(f"å¤±è´¥: {results['failed']}")
    print(f"å¾—åˆ†: {score:.1f}%")
    
    print("\nğŸ“Š åˆ†ç±»æˆç»©:")
    for ptype, stats in results["by_type"].items():
        cat_score = (stats["passed"] / stats["total"]) * 100
        bar = "â–ˆ" * int(cat_score / 10) + "â–‘" * (10 - int(cat_score / 10))
        print(f"  {ptype:18s}: [{bar}] {cat_score:5.1f}%")
    
    # è¯„çº§
    print("\n" + "="*70)
    print("ğŸ¯ æœ€ç»ˆè¯„çº§")
    print("="*70)
    
    if score >= 95:
        rating = "ğŸ¦„ Unicorn Mode"
    elif score >= 90:
        rating = "ğŸ”¥ God Mode"
    elif score >= 80:
        rating = "ğŸ¦ Expert+"
    else:
        rating = "ğŸ† Expert"
    
    print(f"\nå¾—åˆ†: {score:.1f}%")
    print(f"è¯„çº§: {rating}")
    
    # è¿›åŒ–
    print("\n" + "="*70)
    print("ğŸ“ˆ è¿›åŒ–è½¨è¿¹")
    print("="*70)
    
    print("\nv11.0 (æé™ç‰ˆ): 11.8%")
    print("v12.0 (çŸ¥è¯†å¢å¼ºç‰ˆ): {:.1f}%".format(score))
    print(f"æå‡: +{score-11.8:.1f}%")
    
    print("\n" + "="*70)
    
    return score


if __name__ == "__main__":
    run_benchmark()
