#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ RAGçŸ¥è¯†åº“è¿æ¥ç³»ç»Ÿ
=====================
æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation)

åŠŸèƒ½:
1. çŸ¥è¯†åº“ç®¡ç† (æ·»åŠ /æŸ¥è¯¢/åˆ é™¤)
2. æ–‡æœ¬å‘é‡åŒ– (TF-IDFç®€åŒ–ç‰ˆ)
3. ç›¸ä¼¼åº¦æ£€ç´¢
4. ä¸Šä¸‹æ–‡å¢å¼º

Version: 1.0
Date: 2026-02-11
"""

import json
import re
import math
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import Counter, defaultdict
import hashlib


@dataclass
class Document:
    """æ–‡æ¡£"""
    id: str
    content: str
    metadata: Dict = field(default_factory=dict)
    embedding: Optional[List[float]] = None


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    document: Document
    score: float
    snippet: str


class SimpleVectorizer:
    """
    ç®€åŒ–ç‰ˆTF-IDFå‘é‡åŒ–å™¨
    """
    
    def __init__(self):
        self.vocabulary: Dict[str, int] = {}
        self.idf: Dict[str, float] = {}
        
    def fit(self, documents: List[str]):
        """æ„å»ºè¯æ±‡è¡¨å’ŒIDF"""
        # åˆ†è¯
        all_words = []
        for doc in documents:
            words = self._tokenize(doc)
            all_words.extend(words)
        
        # æ„å»ºè¯æ±‡è¡¨
        word_counts = Counter(all_words)
        self.vocabulary = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}
        
        # è®¡ç®—IDF
        n = len(documents)
        for word in self.vocabulary:
            df = sum(1 for doc in documents if word in doc.lower())
            self.idf[word] = math.log(n / (1 + df)) + 1
        
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        text = text.lower()
        words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]+\b', text)
        return words
    
    def transform(self, text: str) -> List[float]:
        """è½¬æ¢ä¸ºTF-IDFå‘é‡"""
        words = self._tokenize(text)
        word_counts = Counter(words)
        
        tfidf = []
        for word, idx in self.vocabulary.items():
            tf = word_counts.get(word, 0) / len(words) if words else 0
            idf = self.idf.get(word, 1.0)
            tfidf.append(tf * idf)
        
        return tfidf
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2:
            return 0.0
        
        dot = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 * norm2 == 0:
            return 0.0
        
        return dot / (norm1 * norm2)


class KnowledgeBase:
    """
    çŸ¥è¯†åº“ç³»ç»Ÿ
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self.documents: Dict[str, Document] = {}
        self.vectorizer = SimpleVectorizer()
        self.built = False
        
    def add_document(self, content: str, metadata: Dict = None) -> str:
        """æ·»åŠ æ–‡æ¡£"""
        doc_id = hashlib.md5(content.encode()).hexdigest()[:8]
        
        doc = Document(
            id=doc_id,
            content=content,
            metadata=metadata or {}
        )
        
        self.documents[doc_id] = doc
        self.built = False  # éœ€è¦é‡æ–°æ„å»º
        
        return doc_id
    
    def add_documents(self, docs: List[Dict[str, str]]) -> List[str]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        ids = []
        for doc in docs:
            doc_id = self.add_document(doc["content"], doc.get("metadata"))
            ids.append(doc_id)
        return ids
    
    def build(self):
        """æ„å»ºç´¢å¼•"""
        if not self.documents:
            return
        
        contents = [doc.content for doc in self.documents.values()]
        self.vectorizer.fit(contents)
        
        for doc in self.documents.values():
            doc.embedding = self.vectorizer.transform(doc.content)
        
        self.built = True
        print(f"  [KnowledgeBase] æ„å»ºå®Œæˆ: {len(self.documents)} æ–‡æ¡£")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[RetrievalResult]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.built:
            self.build()
        
        if not self.documents:
            return []
        
        # æŸ¥è¯¢å‘é‡åŒ–
        query_vec = self.vectorizer.transform(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        results = []
        for doc in self.documents.values():
            if doc.embedding is None:
                continue
                
            score = self.vectorizer.cosine_similarity(query_vec, doc.embedding)
            
            # æå–snippet
            snippet = self._extract_snippet(doc.content, query)
            
            results.append(RetrievalResult(
                document=doc,
                score=score,
                snippet=snippet
            ))
        
        # æ’åºè¿”å›top_k
        results.sort(key=lambda x: x.score, reverse=True)
        
        return results[:top_k]
    
    def _extract_snippet(self, content: str, query: str) -> str:
        """æå–ç›¸å…³ç‰‡æ®µ"""
        # ç®€å•å®ç°ï¼šè¿”å›å‰100å­—
        return content[:100] + "..." if len(content) > 100 else content
    
    def query(self, question: str) -> str:
        """
        ç®€å•é—®ç­”
        
        Args:
            question: é—®é¢˜
            
        Returns:
            ç›¸å…³çŸ¥è¯†
        """
        results = self.retrieve(question, top_k=1)
        
        if results:
            return f"ç›¸å…³çŸ¥è¯†: {results[0].snippet}"
        
        return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†"


class ReasoningKnowledgeBase:
    """
    æ¨ç†çŸ¥è¯†åº“ - ä¸“é—¨ç”¨äºå­˜å‚¨æ¨ç†è§„åˆ™å’Œæ¨¡å¼
    """
    
    def __init__(self):
        self.kb = KnowledgeBase("reasoning")
        self._init_reasoning_knowledge()
    
    def _init_reasoning_knowledge(self):
        """åˆå§‹åŒ–æ¨ç†çŸ¥è¯†"""
        knowledge = [
            {
                "content": "çŸ›ç›¾å…³ç³»: Aå’ŒÂ¬Aå¿…æœ‰ä¸€çœŸä¸€å‡ï¼Œä¸èƒ½åŒçœŸæˆ–åŒå‡",
                "metadata": {"type": "logic", "category": "contradiction"}
            },
            {
                "content": "è•´å«å…³ç³»: Aâ†’Bï¼Œåªæœ‰å½“AçœŸBå‡æ—¶ï¼Œæ•´ä¸ªè•´å«æ‰ä¸ºå‡",
                "metadata": {"type": "logic", "category": "implication"}
            },
            {
                "content": "ç­‰å·®æ•°åˆ—é€šé¡¹å…¬å¼: an = a1 + (n-1)dï¼Œå…¶ä¸­a1ä¸ºé¦–é¡¹ï¼Œdä¸ºå…¬å·®",
                "metadata": {"type": "math", "category": "sequence"}
            },
            {
                "content": "ç­‰æ¯”æ•°åˆ—é€šé¡¹å…¬å¼: an = a1 Ã— r^(n-1)ï¼Œå…¶ä¸­rä¸ºå…¬æ¯”",
                "metadata": {"type": "math", "category": "sequence"}
            },
            {
                "content": "ç©·ä¸¾æ³•: é€ä¸€éªŒè¯æ‰€æœ‰å¯èƒ½æ€§ï¼Œæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„è§£",
                "metadata": {"type": "method", "category": "exhaustive"}
            },
            {
                "content": "åè¯æ³•: å‡è®¾ç»“è®ºä¸æˆç«‹ï¼Œæ¨å¯¼å‡ºçŸ›ç›¾ï¼Œä»è€Œè¯æ˜åŸç»“è®ºæˆç«‹",
                "metadata": {"type": "method", "category": "proof"}
            },
            {
                "content": "å½’è°¬æ³•: é€šè¿‡å‡è®¾æ¨ç†å¯¼å‡ºè’è°¬ç»“è®ºï¼Œä»è€Œå¦å®šå‡è®¾",
                "metadata": {"type": "method", "category": "proof"}
            },
            {
                "content": "è¿é”æ¨ç†: å¦‚æœAâ†’Bä¸”Bâ†’Cï¼Œåˆ™Aâ†’C",
                "metadata": {"type": "logic", "category": "chain"}
            },
            {
                "content": "å……åˆ†æ¡ä»¶: Aæ˜¯Bçš„å……åˆ†æ¡ä»¶æ„å‘³ç€Aæˆç«‹åˆ™Bä¸€å®šæˆç«‹",
                "metadata": {"type": "logic", "category": "condition"}
            },
            {
                "content": "å¿…è¦æ¡ä»¶: Aæ˜¯Bçš„å¿…è¦æ¡ä»¶æ„å‘³ç€Bæˆç«‹åˆ™Aä¸€å®šæˆç«‹",
                "metadata": {"type": "logic", "category": "condition"}
            }
        ]
        
        self.kb.add_documents(knowledge)
        self.kb.build()
    
    def query(self, question: str) -> List[RetrievalResult]:
        """æŸ¥è¯¢ç›¸å…³æ¨ç†çŸ¥è¯†"""
        return self.kb.retrieve(question, top_k=3)
    
    def get_logic_rules(self) -> List[str]:
        """è·å–æ‰€æœ‰é€»è¾‘è§„åˆ™"""
        results = self.kb.retrieve("çŸ›ç›¾ è•´å« æ¨ç†", top_k=10)
        return [r.snippet for r in results]


class RAGEngine:
    """
    RAGæ£€ç´¢å¢å¼ºç”Ÿæˆå¼•æ“
    """
    
    def __init__(self):
        self.reasoning_kb = ReasoningKnowledgeBase()
        self.custom_kb = KnowledgeBase("custom")
        
    def enhance_query(self, question: str) -> Dict[str, Any]:
        """
        å¢å¼ºæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«åŸå§‹é—®é¢˜å’Œæ£€ç´¢çŸ¥è¯†çš„å­—å…¸
        """
        # æ£€ç´¢æ¨ç†çŸ¥è¯†
        reasoning_results = self.reasoning_kb.query(question)
        
        # æ£€ç´¢è‡ªå®šä¹‰çŸ¥è¯†
        custom_results = self.custom_kb.retrieve(question, top_k=2)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = ["ã€æ¨ç†çŸ¥è¯†åº“ã€‘"]
        for r in reasoning_results:
            context_parts.append(f"â€¢ {r.snippet} (ç›¸å…³æ€§: {r.score:.2f})")
        
        if custom_results:
            context_parts.append("\nã€è‡ªå®šä¹‰çŸ¥è¯†åº“ã€‘")
            for r in custom_results:
                context_parts.append(f"â€¢ {r.snippet}")
        
        context = "\n".join(context_parts)
        
        return {
            "question": question,
            "context": context,
            "reasoning_knowledge": [r.snippet for r in reasoning_results],
            "custom_knowledge": [r.snippet for r in custom_results],
            "retrieved_count": len(reasoning_results) + len(custom_results)
        }
    
    def add_knowledge(self, content: str, category: str = "general"):
        """æ·»åŠ è‡ªå®šä¹‰çŸ¥è¯†"""
        self.custom_kb.add_document(
            content,
            metadata={"category": category}
        )
        self.custom_kb.build()
    
    def answer(self, question: str) -> str:
        """
        åŸºäºçŸ¥è¯†çš„é—®ç­”
        
        Args:
            question: é—®é¢˜
            
        Returns:
            ç­”æ¡ˆ
        """
        enhanced = self.enhance_query(question)
        
        if not enhanced["reasoning_knowledge"]:
            return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚"
        
        # ç®€å•å›ç­”ï¼šç»“åˆçŸ¥è¯†å’Œé—®é¢˜
        answer_parts = ["æ ¹æ®æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼š"]
        for i, knowledge in enumerate(enhanced["reasoning_knowledge"][:2], 1):
            answer_parts.append(f"{i}. {knowledge}")
        
        answer_parts.append(f"\nç›¸å…³åº¦: {enhanced['retrieved_count']} æ¡")
        
        return "\n".join(answer_parts)


def demo():
    """æ¼”ç¤º"""
    print("="*70)
    print("ğŸ¦ RAGçŸ¥è¯†åº“ç³»ç»Ÿæ¼”ç¤º")
    print("="*70)
    
    # åˆ›å»ºRAGå¼•æ“
    rag = RAGEngine()
    
    # æŸ¥è¯¢é€»è¾‘é¢˜
    print("\nã€é—®é¢˜1ã€‘ç”²è¯´'æˆ‘ä¼š'ï¼Œä¸™è¯´'ç”²ä¸ä¼š'ï¼Œè¿™æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ")
    result = rag.enhance_query("çŸ›ç›¾å…³ç³» çœŸè¯ å‡è¯")
    print(f"\næ£€ç´¢ç»“æœ:")
    for knowledge in result["reasoning_knowledge"]:
        print(f"  â€¢ {knowledge}")
    
    # æŸ¥è¯¢æ•°å­¦é¢˜
    print("\nã€é—®é¢˜2ã€‘ç­‰å·®æ•°åˆ—å¦‚ä½•è®¡ç®—ï¼Ÿ")
    answer = rag.answer("ç­‰å·®æ•°åˆ— é€šé¡¹å…¬å¼")
    print(f"\nå›ç­”:\n{answer}")
    
    # æ·»åŠ è‡ªå®šä¹‰çŸ¥è¯†
    print("\nã€æ·»åŠ è‡ªå®šä¹‰çŸ¥è¯†ã€‘")
    rag.add_knowledge("å°çˆªæ˜¯ä¸€åªAIåŠ©æ‰‹ï¼Œæ“…é•¿æ¨ç†å’Œåˆ†æ", "identity")
    print("  å·²æ·»åŠ : å°çˆªçš„èº«ä»½ä¿¡æ¯")
    
    # æŸ¥è¯¢è‡ªå®šä¹‰çŸ¥è¯†
    print("\nã€æŸ¥è¯¢è‡ªå®šä¹‰çŸ¥è¯†ã€‘")
    result = rag.enhance_query("å°çˆª æ˜¯è°")
    print(f"  è‡ªå®šä¹‰çŸ¥è¯†: {result['custom_knowledge']}")
    
    print("\n" + "="*70)
    print("âœ… RAGçŸ¥è¯†åº“ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ")
    print("="*70)


if __name__ == "__main__":
    demo()
