#!/usr/bin/env python3
"""
å°çˆªç³»ç»Ÿä¼˜åŒ–ç‰ˆ v2.1 - åŸºäºç¡¬ä»¶æ¡ä»¶çš„æ€§èƒ½ä¼˜åŒ–
æ”¯æŒ: æ‰¹å¤„ç†ã€å¤šçº§ç¼“å­˜ã€å†…å­˜ä¼˜åŒ–ã€å¼‚æ­¥å¹¶å‘ã€æ€§èƒ½ç›‘æ§
"""

import asyncio
import aiohttp
import time
import hashlib
import pickle
import os
import sys
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from collections import OrderedDict
from datetime import datetime
import json

# ==================== ä¼˜åŒ–é…ç½® ====================

@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    # æ‰¹å¤„ç†é…ç½®
    batch_size: int = 4
    batch_timeout: float = 0.05  # 50ms è¶…æ—¶
    
    # ç¼“å­˜é…ç½®
    l1_cache_size: int = 100  # L1 å†…å­˜ç¼“å­˜å¤§å°
    l2_cache_dir: str = "/tmp/clawlet_cache"  # L2 ç£ç›˜ç¼“å­˜ç›®å½•
    cache_ttl: int = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸ (ç§’)
    
    # å†…å­˜é…ç½®
    max_memory_usage: float = 0.7  # æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
    gc_interval: int = 100  # GC é—´éš”
    
    # å¹¶å‘é…ç½®
    max_concurrent: int = 2  # æœ€å¤§å¹¶å‘æ•° (åŸºäº 2CPU æ ¸)
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_interval: float = 10.0  # æŒ‡æ ‡æ”¶é›†é—´éš”


# ==================== å†…å­˜ä¼˜åŒ– ====================

class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_usage: float = 0.7):
        self.max_usage = max_usage
        self.gc_count = 0
        
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ç‡"""
        try:
            with open('/proc/memory_info', 'r') as f:
                lines = f.readlines()
                
                total = 0
                available = 0
                
                for line in lines:
                    if line.startswith('MemTotal:'):
                        total = int(line.split()[1]) * 1024  # KB
                    elif line.startswith('MemAvailable:'):
                        available = int(line.split()[1]) * 1024  # KB
                
                if total > 0:
                    return (total - available) / total
        
        except:
            pass
        
        return 0.5  # é»˜è®¤ 50%
    
    def should_gc(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ GC"""
        return self.get_memory_usage() > self.max_usage
    
    def optimize(self):
        """æ‰§è¡Œå†…å­˜ä¼˜åŒ–"""
        if self.should_gc():
            import gc
            gc.collect()
            self.gc_count += 1
            
            # æ¸…ç†å†…å­˜
            if hasattr(sys, 'set_int_max_str_digits'):
                pass  # Python 3.14+ ä¼˜åŒ–
            
            return True
        return False
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        return {
            'usage_percent': self.get_memory_usage() * 100,
            'gc_count': self.gc_count
        }


# ==================== LRU ç¼“å­˜ ====================

class LRUCache:
    """LRU ç¼“å­˜å®ç°"""
    
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()  # {key: (value, timestamp)}
    
    def _is_expired(self, timestamp: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        return time.time() - timestamp > self.ttl
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            
            # æ£€æŸ¥è¿‡æœŸ
            if self._is_expired(timestamp):
                del self.cache[key]
                return None
            
            # ç§»åŠ¨åˆ°æœ«å°¾ (LRU)
            self.cache.move_to_end(key)
            return value
        
        return None
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        # ç§»é™¤è¿‡æœŸé¡¹
        if key in self.cache:
            del self.cache[key]
        
        # æ£€æŸ¥å®¹é‡
        while len(self.cache) >= self.max_size:
            # ç§»é™¤æœ€æ—©çš„
            self.cache.popitem(last=False)
        
        # æ·»åŠ æ–°é¡¹
        self.cache[key] = (value, time.time())
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'ttl': self.ttl
        }


# ==================== å¤šçº§ç¼“å­˜ç³»ç»Ÿ ====================

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç³»ç»Ÿ (L1 å†…å­˜ + L2 ç£ç›˜)"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        
        # L1 ç¼“å­˜ (å†…å­˜)
        self.l1_cache = LRUCache(
            max_size=config.l1_cache_size,
            ttl=config.cache_ttl
        )
        
        # L2 ç¼“å­˜ (ç£ç›˜)
        self.l2_cache_dir = config.l2_cache_dir
        os.makedirs(self.l2_cache_dir, exist_ok=True)
        
        # ç»Ÿè®¡
        self.l1_hits = 0
        self.l2_hits = 0
        self.misses = 0
    
    def _hash_key(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®çš„å“ˆå¸Œ"""
        return hashlib.md5(key.encode()).hexdigest()
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        # L1 æ£€æŸ¥
        value = self.l1_cache.get(key)
        if value is not None:
            self.l1_hits += 1
            return value
        
        # L2 æ£€æŸ¥
        l2_key = self._hash_key(key)
        l2_path = f"{self.l2_cache_dir}/{l2_key}.pkl"
        
        if os.path.exists(l2_path):
            try:
                with open(l2_path, 'rb') as f:
                    value = pickle.load(f)
                
                # æå‡åˆ° L1
                self.l1_cache.set(key, value)
                self.l2_hits += 1
                return value
            except:
                pass
        
        self.misses += 1
        return None
    
    async def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        # ä¼˜å…ˆè®¾ç½®åˆ° L1
        self.l1_cache.set(key, value)
        
        # L1 æ»¡æ—¶å†™å…¥ L2
        l1_stats = self.l1_cache.get_stats()
        if l1_stats['size'] >= l1_stats['max_size'] * 0.9:  # L1 æ¥è¿‘æ»¡
            # è·å–æœ€æ—©çš„é¡¹
            if self.l1_cache.cache:
                oldest_key, (oldest_value, _) = self.l1_cache.cache.popitem(last=False)
                
                # å†™å…¥ L2
                l2_key = self._hash_key(oldest_key)
                l2_path = f"{self.l2_cache_dir}/{l2_key}.pkl"
                
                try:
                    with open(l2_path, 'wb') as f:
                        pickle.dump(oldest_value, f)
                except:
                    pass
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        total = self.l1_hits + self.l2_hits + self.misses
        l1_rate = (self.l1_hits / max(total, 1)) * 100
        l2_rate = (self.l2_hits / max(total, 1)) * 100
        miss_rate = (self.misses / max(total, 1)) * 100
        
        return {
            'l1_size': self.l1_cache.get_stats()['size'],
            'l1_hits': self.l1_hits,
            'l2_hits': self.l2_hits,
            'misses': self.misses,
            'l1_hit_rate': f"{l1_rate:.1f}%",
            'l2_hit_rate': f"{l2_rate:.1f}%",
            'miss_rate': f"{miss_rate:.1f}%"
        }


# ==================== åŠ¨æ€æ‰¹å¤„ç†å™¨ ====================

class DynamicBatcher:
    """åŠ¨æ€æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.queue = []
        self.last_batch_time = time.time()
        self.total_batches = 0
        self.total_requests = 0
        self.batch_times = []
    
    async def add_request(self, request: Dict) -> Optional[List[Dict]]:
        """æ·»åŠ è¯·æ±‚ï¼Œè¿”å›æ‰¹é‡ç»“æœ"""
        self.queue.append({
            'request': request,
            'timestamp': time.time()
        })
        self.total_requests += 1
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰¹å¤„ç†æ¡ä»¶
        if len(self.queue) >= self.config.batch_size:
            return await self._process_batch()
        
        # æ£€æŸ¥è¶…æ—¶
        elapsed = time.time() - self.last_batch_time
        if elapsed >= self.config.batch_timeout:
            if self.queue:  # è‡³å°‘æœ‰ä¸€ä¸ªè¯·æ±‚
                return await self._process_batch()
        
        return None
    
    async def _process_batch(self) -> List[Dict]:
        """å¤„ç†æ‰¹é‡è¯·æ±‚"""
        if not self.queue:
            return []
        
        batch_start = time.time()
        batch = self.queue
        self.queue = []
        self.last_batch_time = time.time()
        self.total_batches += 1
        
        # æ¨¡æ‹Ÿæ‰¹å¤„ç† (å®é™…åº”ç”¨ä¸­ä¼šè°ƒç”¨æ¨¡å‹)
        results = []
        for item in batch:
            results.append({
                'request_id': item['request'].get('id', 'unknown'),
                'result': f"Processed: {item['request'].get('text', 'N/A')[:50]}",
                'batch_index': len(results)
            })
        
        batch_time = time.time() - batch_start
        self.batch_times.append(batch_time)
        
        return results
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        avg_batch_time = sum(self.batch_times) / max(len(self.batch_times), 1)
        
        return {
            'total_batches': self.total_batches,
            'total_requests': self.total_requests,
            'queue_size': len(self.queue),
            'avg_batch_time': f"{avg_batch_time*1000:.2f}ms",
            'avg_batch_size': self.total_requests / max(self.total_batches, 1)
        }


# ==================== å¼‚æ­¥å¹¶å‘æ§åˆ¶å™¨ ====================

class ConcurrencyController:
    """å¹¶å‘æ§åˆ¶å™¨"""
    
    def __init__(self, max_concurrent: int = 2):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.active_tasks = 0
        self.total_tasks = 0
        self.completed_tasks = 0
        self.total_latency = 0.0
    
    async def execute(self, coro) -> Any:
        """æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ (å¸¦å¹¶å‘æ§åˆ¶)"""
        async with self.semaphore:
            start = time.time()
            self.active_tasks += 1
            self.total_tasks += 1
            
            try:
                result = await coro
                self.completed_tasks += 1
                latency = time.time() - start
                self.total_latency += latency
                return result
            except Exception as e:
                self.completed_tasks += 1
                raise e
            finally:
                self.active_tasks -= 1
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        return {
            'active': self.active_tasks,
            'total': self.total_tasks,
            'completed': self.completed_tasks,
            'avg_latency': f"{self.total_latency / max(self.completed_tasks, 1)*1000:.2f}ms",
            'throughput': self.completed_tasks / max(self.total_latency, 0.001)
        }


# ==================== API è¿æ¥æ±  ====================

class ConnectionPool:
    """API è¿æ¥æ± """
    
    def __init__(self, max_connections: int = 5, max_per_host: int = 2):
        self.connector = aiohttp.TCPConnector(
            limit=max_connections,
            limit_per_host=max_per_host,
            ttl_dns_cache=300,
            keepalive_timeout=30
        )
        self.session = None
        self.request_count = 0
        self.error_count = 0
    
    async def get_session(self) -> aiohttp.ClientSession:
        """è·å–ä¼šè¯"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession(
                connector=self.connector,
                timeout=aiohttp.ClientTimeout(total=30)
            )
        return self.session
    
    async def request(self, method: str, url: str, **kwargs) -> Dict:
        """å‘èµ·è¯·æ±‚"""
        try:
            session = await self.get_session()
            async with session.request(method, url, **kwargs) as response:
                self.request_count += 1
                return {
                    'status': response.status,
                    'data': await response.json() if response.headers.get('content-type', '').startswith('application/json') else await response.text()
                }
        except Exception as e:
            self.error_count += 1
            return {'error': str(e)}
    
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.session and not self.session.closed:
            await self.session.close()
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        total = self.request_count + self.error_count
        error_rate = (self.error_count / max(total, 1)) * 100
        
        return {
            'requests': self.request_count,
            'errors': self.error_count,
            'error_rate': f"{error_rate:.1f}%"
        }


# ==================== æ€§èƒ½ç›‘æ§å™¨ ====================

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.metrics = {
            'requests': 0,
            'successful': 0,
            'failed': 0,
            'total_latency': 0.0,
            'min_latency': float('inf'),
            'max_latency': 0.0,
            'batch_count': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        self.latencies = []
    
    def record(self, success: bool, latency: float, cached: bool = False, batched: bool = False):
        """è®°å½•æŒ‡æ ‡"""
        self.metrics['requests'] += 1
        
        if success:
            self.metrics['successful'] += 1
        else:
            self.metrics['failed'] += 1
        
        self.metrics['total_latency'] += latency
        self.latencies.append(latency)
        
        if latency < self.metrics['min_latency']:
            self.metrics['min_latency'] = latency
        
        if latency > self.metrics['max_latency']:
            self.metrics['max_latency'] = latency
        
        if cached:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        uptime = time.time() - self.start_time
        total = self.metrics['requests']
        successful = self.metrics['successful']
        
        # è®¡ç®— P50, P90, P99
        sorted_latencies = sorted(self.latencies) if self.latencies else [0]
        p50 = sorted_latencies[int(len(sorted_latencies) * 0.50)]
        p90 = sorted_latencies[int(len(sorted_latencies) * 0.90)]
        p99 = sorted_latencies[int(len(sorted_latencies) * 0.99)]
        
        return {
            'uptime': f"{uptime:.1f}s",
            'total_requests': total,
            'successful': successful,
            'failed': self.metrics['failed'],
            'success_rate': f"{(successful / max(total, 1)) * 100:.1f}%",
            'latency': {
                'avg': f"{(self.metrics['total_latency'] / max(total, 1)) * 1000:.2f}ms",
                'min': f"{self.metrics['min_latency'] * 1000:.2f}ms",
                'max': f"{self.metrics['max_latency'] * 1000:.2f}ms",
                'p50': f"{p50 * 1000:.2f}ms",
                'p90': f"{p90 * 1000:.2f}ms",
                'p99': f"{p99 * 1000:.2f}ms"
            },
            'cache': {
                'hits': self.metrics['cache_hits'],
                'misses': self.metrics['cache_misses'],
                'hit_rate': f"{(self.metrics['cache_hits'] / max(self.metrics['cache_hits'] + self.metrics['cache_misses'], 1)) * 100:.1f}%"
            },
            'throughput': f"{total / max(uptime, 0.001):.2f} req/s"
        }


# ==================== ä¼˜åŒ–åçš„å°çˆªç³»ç»Ÿ ====================

class ClawletOptimized:
    """ä¼˜åŒ–åçš„å°çˆªç³»ç»Ÿ"""
    
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        
        # åˆå§‹åŒ–å„ä¼˜åŒ–ç»„ä»¶
        self.memory_optimizer = MemoryOptimizer(max_usage=self.config.max_memory_usage)
        self.cache = MultiLevelCache(self.config)
        self.batcher = DynamicBatcher(self.config)
        self.concurrency = ConcurrencyController(self.config.max_concurrent)
        self.connection_pool = ConnectionPool()
        self.monitor = PerformanceMonitor()
        
        # ç‰ˆæœ¬
        self.version = "v2.1"
        
        # åˆå§‹åŒ–
        self._init_system()
    
    def _init_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¦ å°çˆªç³»ç»Ÿä¼˜åŒ–ç‰ˆ {self.version}")
        print(f"{'='*80}")
        print("\nâœ… åˆå§‹åŒ–ä¼˜åŒ–ç»„ä»¶:")
        print(f"   â€¢ æ‰¹å¤„ç†å™¨ (batch_size={self.config.batch_size})")
        print(f"   â€¢ å¤šçº§ç¼“å­˜ (L1={self.config.l1_cache_size}, L2={self.config.l2_cache_dir})")
        print(f"   â€¢ å†…å­˜ä¼˜åŒ– (max_usage={self.config.max_memory_usage*100}%)")
        print(f"   â€¢ å¹¶å‘æ§åˆ¶ (max_concurrent={self.config.max_concurrent})")
        print(f"   â€¢ æ€§èƒ½ç›‘æ§ (interval={self.config.metrics_interval}s)")
        print(f"\n{'='*80}\n")
    
    async def process_request(self, request: Dict) -> Dict:
        """
        å¤„ç†è¯·æ±‚ (ä¼˜åŒ–ç‰ˆ)
        
        æµç¨‹: ç¼“å­˜æ£€æŸ¥ â†’ æ‰¹å¤„ç† â†’ å¹¶å‘æ‰§è¡Œ â†’ ç¼“å­˜å­˜å‚¨ â†’ ç›‘æ§è®°å½•
        """
        start_time = time.time()
        request_text = request.get('text', '')
        
        # 1. ç¼“å­˜æ£€æŸ¥
        cache_key = hashlib.md5(request_text.encode()).hexdigest()
        cached_result = await self.cache.get(cache_key)
        
        if cached_result:
            latency = time.time() - start_time
            self.monitor.record(
                success=True,
                latency=latency,
                cached=True,
                batched=False
            )
            
            return {
                'result': cached_result,
                'cached': True,
                'latency': latency
            }
        
        # 2. æ‰¹å¤„ç†
        batch_results = await self.batcher.add_request(request)
        
        if batch_results:
            # æ‰¹é‡å¤„ç†å®Œæˆ
            batch_latency = time.time() - start_time
            
            # å­˜å‚¨ç¬¬ä¸€æ‰¹ç»“æœåˆ°ç¼“å­˜
            for item in batch_results:
                result_text = item.get('result', '')
                if result_text:
                    result_key = hashlib.md5(result_text.encode()).hexdigest()
                    await self.cache.set(result_key, result_text)
            
            self.monitor.record(
                success=True,
                latency=batch_latency,
                cached=False,
                batched=True
            )
            
            return {
                'batch_results': batch_results,
                'batch_size': len(batch_results),
                'cached': False,
                'batched': True,
                'latency': batch_latency
            }
        
        # 3. å¹¶å‘æ‰§è¡Œ (å•ä¸ªè¯·æ±‚)
        async def single_infer():
            # æ¨¡æ‹Ÿæ¨ç†
            await asyncio.sleep(0.01)  # 10ms æ¨¡æ‹Ÿå»¶è¿Ÿ
            return f"Processed: {request_text[:100]}"
        
        result = await self.concurrency.execute(single_infer())
        
        # 4. å­˜å‚¨åˆ°ç¼“å­˜
        await self.cache.set(cache_key, result)
        
        latency = time.time() - start_time
        self.monitor.record(
            success=True,
            latency=latency,
            cached=False,
            batched=False
        )
        
        return {
            'result': result,
            'cached': False,
            'batched': False,
            'latency': latency
        }
    
    def get_system_stats(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'version': self.version,
            'memory': self.memory_optimizer.get_stats(),
            'cache': self.cache.get_stats(),
            'batcher': self.batcher.get_stats(),
            'concurrency': self.concurrency.get_stats(),
            'connection_pool': self.connection_pool.get_stats(),
            'performance': self.monitor.get_stats()
        }
    
    async def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        memory_stats = self.memory_optimizer.get_stats()
        
        return {
            'status': 'healthy' if memory_stats['usage_percent'] < 90 else 'warning',
            'memory_usage': f"{memory_stats['usage_percent']:.1f}%",
            'uptime': self.monitor.get_stats()['uptime'],
            'cache_status': self.cache.get_stats()['l1_size'] > 0
        }


# ==================== æ€§èƒ½æµ‹è¯• ====================

async def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "="*80)
    print("ğŸš€ æ€§èƒ½æµ‹è¯•")
    print("="*80 + "\n")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    clawlet = ClawletOptimized()
    
    # æµ‹è¯•è¯·æ±‚
    test_requests = [
        {'id': f'req_{i}', 'text': f'Test request number {i}'}
        for i in range(20)
    ]
    
    print(f"ğŸ“¤ å‘é€ {len(test_requests)} ä¸ªæµ‹è¯•è¯·æ±‚...\n")
    
    # å¹¶å‘å‘é€è¯·æ±‚
    start_time = time.time()
    
    results = []
    for request in test_requests:
        result = await clawlet.process_request(request)
        results.append(result)
        
        # æ˜¾ç¤ºè¿›åº¦
        if len(results) % 5 == 0:
            print(f"  å·²å®Œæˆ: {len(results)}/{len(test_requests)}")
    
    total_time = time.time() - start_time
    
    # ç»Ÿè®¡ç»“æœ
    cached_count = sum(1 for r in results if r.get('cached'))
    batched_count = sum(1 for r in results if r.get('batched'))
    total_latency = sum(r.get('latency', 0) for r in results)
    
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•ç»“æœ")
    print("="*80)
    
    print(f"\nâœ… æˆåŠŸå¤„ç†: {len(results)}/{len(test_requests)}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s")
    print(f"ğŸ“ˆ å¹³å‡å»¶è¿Ÿ: {total_latency/len(results)*1000:.2f}ms")
    print(f"ğŸš€ ååé‡: {len(test_requests)/max(total_time, 0.001):.2f} req/s")
    print(f"\nğŸ’¾ ç¼“å­˜å‘½ä¸­: {cached_count}")
    print(f"ğŸ“¦ æ‰¹å¤„ç†: {batched_count}")
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    print("\n" + "="*80)
    print("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    print("="*80)
    
    stats = clawlet.get_system_stats()
    
    print(f"\nğŸ¦ å°çˆªç³»ç»Ÿ {stats['version']}")
    print(f"\nğŸ’¾ å†…å­˜:")
    print(f"   â€¢ ä½¿ç”¨ç‡: {stats['memory']['usage_percent']:.1f}%")
    print(f"   â€¢ GCæ¬¡æ•°: {stats['memory']['gc_count']}")
    
    print(f"\nğŸ’¾ ç¼“å­˜:")
    print(f"   â€¢ L1 å¤§å°: {stats['cache']['l1_size']}")
    print(f"   â€¢ L1 å‘½ä¸­: {stats['cache']['l1_hits']}")
    print(f"   â€¢ L2 å‘½ä¸­: {stats['cache']['l2_hits']}")
    print(f"   â€¢ å‘½ä¸­ç‡: {stats['cache']['l1_hit_rate']}")
    
    print(f"\nğŸ“¦ æ‰¹å¤„ç†:")
    print(f"   â€¢ æ‰¹æ¬¡æ•°: {stats['batcher']['total_batches']}")
    print(f"   â€¢ å¹³å‡å¤§å°: {stats['batcher']['avg_batch_size']:.1f}")
    
    print(f"\nâš¡ å¹¶å‘:")
    print(f"   â€¢ å®Œæˆä»»åŠ¡: {stats['concurrency']['completed']}")
    print(f"   â€¢ å¹³å‡å»¶è¿Ÿ: {stats['concurrency']['avg_latency']}")
    
    print(f"\nğŸ“ˆ æ€§èƒ½:")
    perf = stats['performance']
    print(f"   â€¢ ååé‡: {perf['throughput']}")
    print(f"   â€¢ æˆåŠŸç‡: {perf['success_rate']}")
    print(f"   â€¢ P50å»¶è¿Ÿ: {perf['latency']['p50']}")
    print(f"   â€¢ P99å»¶è¿Ÿ: {perf['latency']['p99']}")
    
    print("\n" + "="*80)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print("="*80 + "\n")
    
    return clawlet


# ==================== ä¸»å‡½æ•° ====================

if __name__ == "__main__":
    import asyncio
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    asyncio.run(run_performance_test())
